# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNOqiaPRkZogzzONZ0PdoYECbVrXYsS5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

train

test

sub = pd.read_csv('/content/sample_submission.csv')
sub.shape

sub

train.info()

test.info()

print(train.shape)
print(test.shape)

print(train.isnull().sum())
print(test.isnull().sum())

sns.countplot(x='is_promoted', data=train)
plt.title("Promotion Distribution")
plt.show()

"""Preprocessing"""

test_employee_id = test['employee_id']                                          #to svae it for final

train.drop('employee_id', axis=1, inplace=True)
test.drop('employee_id', axis=1, inplace=True)

train['education'].fillna(train['education'].mode()[0], inplace=True)
test['education'].fillna(test['education'].mode()[0], inplace=True)

train['previous_year_rating'].fillna(train['previous_year_rating'].median(), inplace=True)
test['previous_year_rating'].fillna(test['previous_year_rating'].median(), inplace=True)

print(train.isnull().sum())
print(test.isnull().sum())

cat_col = ['department', 'region', 'education', 'gender', 'recruitment_channel']

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
for col in cat_col:
  train[col] = LE.fit_transform(train[col])
  test[col] = LE.fit_transform(test[col])

from sklearn.model_selection import train_test_split
X = train.drop('is_promoted', axis=1)
y = train['is_promoted']

# Split the data into training and testing sets
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 20% Testing 75 and Training (80%)
print(X.shape)
print(y.shape)

print(X_test.shape)
print(y_test.shape)

X_test

print(X_train.shape)
print(y_train.shape)

import joblib

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
LR_pred = model.predict(X_test)

joblib.dump(model,'LR.pkl')

LR_pred.shape

"""kNN"""

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
KNN_pred = model.predict(X_test)

joblib.dump(model,'KNN.pkl')

"""SVM"""

from sklearn.svm import SVC

model = SVC(kernel='linear')
model.fit(X_train, y_train)
SVM_pred = model.predict(X_test)

joblib.dump(model,'SVM.pkl')

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(X_train, y_train)
NB_pred = model.predict(X_test)

joblib.dump(model,'NB.pkl')

"""Decision tree"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
DT_pred = model.predict(X_test)

joblib.dump(model,'DT.pkl')

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=200)
model.fit(X_train, y_train)
RF_pred = model.predict(X_test)

joblib.dump(model,'RF.pkl')

"""MLP"""

from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(10,10,5), max_iter=1000, random_state=42)
model.fit(X_train, y_train)
MLP_pred = model.predict(X_test)

joblib.dump(model,'MLP.pkl')

"""Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
GB_pred = model.predict(X_test)

joblib.dump(model,'GB.pkl')

"""XGB"""

from xgboost import XGBClassifier

model = XGBClassifier(eval_metric='logloss')
model.fit(X_train, y_train)
XGB_pred = model.predict(X_test)

joblib.dump(model,'XGB.pkl')

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

print("Logistic Regression")
print("Accuracy:", accuracy_score(y_test, LR_pred))
print(classification_report(y_test, LR_pred))

cm = confusion_matrix(y_test, LR_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("k-Nearest Neighbors")
print("Accuracy:", accuracy_score(y_test, KNN_pred))
print(classification_report(y_test, KNN_pred))

cm = confusion_matrix(y_test, KNN_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - kNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("Support Vector Machine")
print("Accuracy:", accuracy_score(y_test, SVM_pred))
print(classification_report(y_test, SVM_pred))

cm = confusion_matrix(y_test, SVM_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(" Naive Bayes")
print("Accuracy:", accuracy_score(y_test, NB_pred))
print(classification_report(y_test, NB_pred))

cm = confusion_matrix(y_test, NB_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(" Decision Tree")
print("Accuracy:", accuracy_score(y_test, DT_pred))
print(classification_report(y_test, DT_pred))

cm = confusion_matrix(y_test, DT_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(" Random Forest")
print("Accuracy:", accuracy_score(y_test, RF_pred))
print(classification_report(y_test, RF_pred))

cm = confusion_matrix(y_test, RF_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(" MLP Classifier")
print("Accuracy:", accuracy_score(y_test, MLP_pred))
print(classification_report(y_test, MLP_pred))

cm = confusion_matrix(y_test, MLP_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - MLP Classifier")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(" Gradient Boosting")
print("Accuracy:", accuracy_score(y_test, GB_pred))
print(classification_report(y_test, GB_pred))

cm = confusion_matrix(y_test, GB_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Gradient Boosting")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

len(y_test)

print(" XGBoost Classifier")
print("Accuracy:", accuracy_score(y_test, XGB_pred))
print(classification_report(y_test, XGB_pred))

cm = confusion_matrix(y_test, XGB_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Fine Tuning (HyperParameter Tuning)"""

# kNN hyperparameter tuning
for k in [3, 5, 7, 9]:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    KNN_pred = model.predict(X_test)

    print(f"k-Nearest Neighbors for {k}")
    print("Accuracy:", accuracy_score(y_test, KNN_pred))
    print(classification_report(y_test, KNN_pred))

# SVM hyperparameter tuning
for k in ['linear', 'rbf', 'poly']:
    model = SVC(kernel=k)
    model.fit(X_train, y_train)
    SVC_pred = model.predict(X_test)

    print(f"Support Vector Machine for k={k}")
    print("Accuracy:", accuracy_score(y_test, SVM_pred))
    print(classification_report(y_test, SVM_pred))

# Random-Forest hyperparameter tuning
for n in [50, 100, 150]:
    model = RandomForestClassifier(n_estimators=n)
    model.fit(X_train, y_train)
    RF_pred = model.predict(X_test)

    print(f"Random Forest for n={n}")
    print("Accuracy:", accuracy_score(y_test, RF_pred))
    print(classification_report(y_test, RF_pred))

"""From the above result , XGBoost has the higher Score value"""

# Hyperparameter tuning(automated)
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import f1_score

# Define model
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')

# Parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Grid search with 3-fold CV and F1 scoring
grid_search = GridSearchCV(estimator=xgb_model,
                           param_grid=param_grid,
                           scoring='f1',
                           cv=3,
                           verbose=1,
                           n_jobs=-1)

# Fit on training data
grid_search.fit(X, y)

# Best model
best_model_grid = grid_search.best_estimator_

# Predict on test
test_preds_grid = best_model_grid.predict(test)

# Save to CSV
sub['is_promoted'] = test_preds_grid
sub.to_csv('final_submission_grid.csv', index=False)